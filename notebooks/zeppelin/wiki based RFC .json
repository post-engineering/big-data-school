{"paragraphs":[{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.storage.StorageLevel\n\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature._\n\n    val targetCategories = Array(\n          \"Terror\",\n          \"Islamic state\",\n          \"Bomb\",\n          \"Terract\",\n          \"Attack\",\n          \"Weapon\",\n          \"Drug\",\n          \"Cocaine\",\n          \"Kidnapping\",\n          \"Porn\",\n          \"Crime\",\n          \"Murder\",\n          \"Explosion\"\n        )\n        .toSeq\n        .map(_.toLowerCase)\n\n    println(targetCategories)\n    \n    val trainingData = sc.textFile(\"hdfs://172.26.5.36/analytics/wiki/mapping/*\", 1000)\n        .map { s =>\n            \"\"\"\\(([A-Za-z\\d\\s]{3,60}),\\[(.*)\\]\\)\"\"\".r.findFirstMatchIn(s) match {\n              case Some(m) => {\n                (m.group(1), m.group(2).split(\", \").toSeq)\n    \n              }\n              case None => (\"Unknown\", Seq.empty[String])\n             }\n        }\n        .map { case (k, v) =>\n          val t = {\n            for (c <- targetCategories; if k.toLowerCase.contains(c)) yield c\n          }\n          (if (t.size > 0) t(0) else k, v)\n        }\n        .filter { case (k, v) =>\n            ( k != null && targetCategories.contains(k))\n        }\n        .persist(StorageLevel.DISK_ONLY) //MEMORY_AND_DISK_SER \n        .toDF(\"category\", \"terms\")\n        \n    trainingData.registerTempTable(\"WikiCategotyToArticleTermsMapping\")\n    \n\n    val hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(\"terms\")\n      .setOutputCol(\"tfFeatures\")\n\n    val idf = new IDF()\n      .setInputCol(\"tfFeatures\")\n      .setOutputCol(\"tfidfFeatures\")\n\n    val normalizer = new Normalizer()\n      .setInputCol(\"tfidfFeatures\")\n      .setOutputCol(\"features\")\n      \n    // Index labels, adding metadata to the label column.\n    // Fit on whole dataset to include all labels in index.\n    val labelIndexer = new StringIndexer()\n      .setInputCol(\"category\")\n      .setOutputCol(\"indexedLabel\")\n      .fit(trainingData)\n    \n    // Train a RandomForest model.\n    val rf = new RandomForestClassifier()\n      .setLabelCol(\"indexedLabel\")\n      .setFeaturesCol(\"features\")\n      .setNumTrees(9)\n      .setFeatureSubsetStrategy(\"auto\")\n      .setImpurity(\"gini\")\n      .setMaxDepth(10)\n      .setMaxBins(20)\n    \n    // Convert indexed labels back to original labels.\n    val labelConverter = new IndexToString()\n      .setInputCol(\"prediction\")\n      .setOutputCol(\"predictedLabel\")\n      .setLabels(labelIndexer.labels)\n\n    val pipeline = new Pipeline()\n      .setStages(Array(\n          hashingTF, \n          idf, \n          normalizer, \n          labelIndexer, \n          rf, \n          labelConverter\n          )\n        )\n\n    // Train model\n    val model = pipeline.fit(trainingData)\n    \n    val pathToPdmlData = \"hdfs://172.26.5.36/analytics/pdml-html-tokenized\"  \n    \n    val pdmlDataTermsDF =  sc.textFile(pathToPdmlData)\n        .map(s => (s.hashCode, s.split(\", \")))\n        .toDF(\"id\", \"terms\")\n    \n    // Make predictions\n    model.transform(pdmlDataTermsDF)\n         .select(\"predictedLabel\",\"terms\")\n         .registerTempTable(\"ResultPredictions\")","dateUpdated":"Jan 29, 2016 2:54:28 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1453731452267_-240866395","id":"20160125-171732_2099196001","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature._\ntargetCategories: Seq[String] = ArrayBuffer(terror, islamic state, bomb, terract, attack, weapon, drug, cocaine, kidnapping, porn, crime, murder, explosion)\nArrayBuffer(terror, islamic state, bomb, terract, attack, weapon, drug, cocaine, kidnapping, porn, crime, murder, explosion)\ntrainingData: org.apache.spark.sql.DataFrame = [category: string, terms: array<string>]\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_6433ea15a088\nidf: org.apache.spark.ml.feature.IDF = idf_b1ebae4cec66\nnormalizer: org.apache.spark.ml.feature.Normalizer = normalizer_d60aaa9f8378\nlabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_3d111f8359a7\nrf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_c2f59e2707dc\nlabelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_6d62393df679\npipeline: org.apache.spark.ml.Pipeline = pipeline_e45f26728e9d\nmodel: org.apache.spark.ml.PipelineModel = pipeline_e45f26728e9d\npathToPdmlData: String = hdfs://172.26.5.36/analytics/pdml-html-tokenized\npdmlDataTermsDF: org.apache.spark.sql.DataFrame = [id: int, terms: array<string>]\n"},"dateCreated":"Jan 25, 2016 5:17:32 PM","dateStarted":"Jan 29, 2016 2:54:28 PM","dateFinished":"Jan 29, 2016 3:05:42 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:419"},{"text":"%sql \nselect category, count(terms)\n    from WikiCategotyToArticleTermsMapping\n    group by category\n    -- where category like '%history%'","dateUpdated":"Jan 29, 2016 2:53:40 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"category","index":0,"aggr":"sum"}],"values":[{"name":"_c1","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"category","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1453731452267_-240866395","id":"20160125-171732_2067474190","result":{"code":"ERROR","type":"TEXT","msg":"org.apache.spark.sql.AnalysisException: Table not found: WikiCategotyToArticleTermsMapping; line 2 pos 9\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$9.applyOrElse(Analyzer.scala:315)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$9.applyOrElse(Analyzer.scala:310)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:53)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:300)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:138)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:302)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"},"dateCreated":"Jan 25, 2016 5:17:32 PM","dateStarted":"Jan 29, 2016 2:53:40 PM","dateFinished":"Jan 29, 2016 2:54:15 PM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:420"},{"text":"%sql\nselect predictedLabel, count(terms)\n    from ResultPredictions\n    group by predictedLabel","dateUpdated":"Jan 26, 2016 2:17:23 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{"xAxis":null}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1453731590585_265679340","id":"20160125-171950_1004636069","result":{"code":"SUCCESS","type":"TABLE","msg":"predictedLabel\t_c1\nart\t37330\nscience\t11\ncomputer\t51\nhistory\t179\n","comment":"","msgTable":[[{"key":"_c1","value":"art"},{"key":"_c1","value":"37330"}],[{"value":"science"},{"value":"11"}],[{"value":"computer"},{"value":"51"}],[{"value":"history"},{"value":"179"}]],"columnNames":[{"name":"predictedLabel","index":0,"aggr":"sum"},{"name":"_c1","index":1,"aggr":"sum"}],"rows":[["art","37330"],["science","11"],["computer","51"],["history","179"]]},"dateCreated":"Jan 25, 2016 5:19:50 PM","dateStarted":"Jan 26, 2016 2:17:23 PM","dateFinished":"Jan 26, 2016 2:18:00 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:421"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1453803943507_1665658976","id":"20160126-132543_990000891","dateCreated":"Jan 26, 2016 1:25:43 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:422"}],"name":"wiki based RFC ","id":"2BAV1GKFN","angularObjects":{"2B859DYW2":[],"2BB8DM8TE":[],"2BBM1C5SF":[],"2B9GW7GB6":[],"2B8PKUN47":[],"2BA1SUK3F":[],"2BAMRBP13":[],"2B99PJYUX":[],"2B9GMPMFC":[],"2B8J7AV91":[],"2B8GHRN7M":[],"2BAJTXXX4":[],"2BB1SGDSP":[],"2B9EE34X6":[],"2BAXYW7FV":[]},"config":{"looknfeel":"default"},"info":{}}