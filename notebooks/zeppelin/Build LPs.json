{"paragraphs":[{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.ml.classification.{RandomForestClassifier}\n\n val categoryToArticleMappingDF = sc.textFile(\"hdfs://172.26.5.36/analytics/wiki/mapping/*\", 1000)\n        .map { s =>\n            \"\"\"\\(([A-Za-z\\d\\s]{3,60}),\\[(.*)\\]\\)\"\"\".r.findFirstMatchIn(s) match {\n              case Some(m) => {\n                (m.group(1), m.group(2).split(\", \").toSeq)\n    \n              }\n              case None => (\"Unknown\", Seq.empty[String])\n             }\n        }\n        .filter{ case(k,v) => \n                    (k.split(\" \").size == 1)\n        }\n        .toDF(\"category\", \"terms\")\n        .persist(StorageLevel.DISK_ONLY) //\n \n    val indexer = new StringIndexer()\n      .setInputCol(\"category\")\n      .setOutputCol(\"label\")\n\n    val hashingTF = new HashingTF()\n      .setNumFeatures(100)\n      .setInputCol(\"terms\")\n      .setOutputCol(\"tfFeatures\")\n\n    val idf = new IDF()\n      .setInputCol(\"tfFeatures\")\n      .setOutputCol(\"tfidfFeatures\")\n\n    val normalizer = new Normalizer()\n      .setInputCol(\"tfidfFeatures\")\n      .setOutputCol(\"normalizedFeatures\")\n\n    //no need for RF\n    /* val scaler = new StandardScaler()\n      .setInputCol(\"normalizedFeatures\")\n      .setOutputCol(\"scaledFeatures\")*/ \n    \n        val rf = new RandomForestClassifier()\n          .setNumTrees(3)\n          .setFeatureSubsetStrategy(\"auto\")\n          .setImpurity(\"gini\")\n          .setCacheNodeIds(false)\n          .setMaxBins(30000)\n          .setMaxMemoryInMB(3000)\n          .setFeaturesCol(\"normalizedFeatures\")\n          // .setMaxDepth(5) default\n\n    val pipeline = new Pipeline()\n      .setStages(Array(\n        indexer,\n        hashingTF,\n        idf,\n        normalizer,\n        rf)\n      )\n      \n    // Fit the pipeline to training documents.\n    val model = pipeline.fit(categoryToArticleMappingDF)\n   // model.write.overwrite().save(\"file:////home/ipertushin/IdeaProjects/pornocluster/data/wiki/rfc-model\")\n","dateUpdated":"Feb 8, 2016 10:08:05 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1453303792908_-1980061175","id":"20160120-182952_1065279525","result":{"code":"ERROR","type":"TEXT","msg":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.ml.classification.RandomForestClassifier\ncategoryToArticleMappingDF: org.apache.spark.sql.DataFrame = [category: string, terms: array<string>]\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_e1f5f988677b\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_0079c96802d2\nidf: org.apache.spark.ml.feature.IDF = idf_5fb63285f184\nnormalizer: org.apache.spark.ml.feature.Normalizer = normalizer_1d4ce6835922\nrf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_9ea1e3be8fab\npipeline: org.apache.spark.ml.Pipeline = pipeline_fa5aab7f15a0\njava.lang.IllegalArgumentException: requirement failed: RandomForest/DecisionTree given maxMemoryInMB = 3000, which is too small for the given features.  Minimum value = 61917\n\tat scala.Predef$.require(Predef.scala:233)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:113)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:111)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:43)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:90)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:71)\n\tat org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:144)\n\tat org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:140)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:42)\n\tat scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:43)\n\tat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:140)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:72)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:74)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:76)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:78)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n\tat $iwC$$iwC$$iwC.<init>(<console>:92)\n\tat $iwC$$iwC.<init>(<console>:94)\n\tat $iwC.<init>(<console>:96)\n\tat <init>(<console>:98)\n\tat .<init>(<console>:102)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:713)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:678)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:671)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:302)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"},"dateCreated":"Jan 20, 2016 6:29:52 PM","dateStarted":"Jan 26, 2016 5:21:48 PM","dateFinished":"Jan 26, 2016 5:29:56 PM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1453304236559_-1714723996","id":"20160120-183716_864986860","dateCreated":"Jan 20, 2016 6:37:16 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:81"}],"name":"Build LPs","id":"2BAGN3KFC","angularObjects":{"2B859DYW2":[],"2BB8DM8TE":[],"2BBM1C5SF":[],"2B9GW7GB6":[],"2B8PKUN47":[],"2BA1SUK3F":[],"2BAMRBP13":[],"2B99PJYUX":[],"2B9GMPMFC":[],"2B8J7AV91":[],"2B8GHRN7M":[],"2BAJTXXX4":[],"2BB1SGDSP":[],"2B9EE34X6":[],"2BAXYW7FV":[]},"config":{"looknfeel":"default"},"info":{}}